{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LayerNormalization , Layer , Dense , ReLU , Dropout , Embedding\n",
    "import math\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow import linalg , ones , maximum , newaxis\n",
    "from tensorflow import reshape , transpose , cast , matmul , train ,shape , math , float32 ,equal , argmax ,data ,GradientTape , TensorSpec,function, int64 , reduce_sum \n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow import int64,convert_to_tensor\n",
    "from pickle import load\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def call ( self, x, sublayer_x):\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        return self.layer_norm(add)\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff , d_model , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fullyconnected1=Dense(d_ff)\n",
    "        self.fullyconnected2=Dense(d_model)\n",
    "        self.activation=ReLU()\n",
    "\n",
    "    def call(self,x):\n",
    "        x_fc1=self.fullyconnected1(x)\n",
    "        return self.fullyconnected2(self.activation(x_fc1))\n",
    "\n",
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
    "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim,weights=[word_embedding_matrix],trainable=False)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim,weights=[pos_embedding_matrix],trainable=False)\n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices\n",
    "class GetAttentionScores(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values,d_k, mask):\n",
    "        # Scale dot-product of queries and keys\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True) / math.sqrt(d_k)\n",
    "\n",
    "        # Apply mask if it exists, setting masked positions to a large negative value\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * tf.cast(mask, dtype=scores.dtype)\n",
    "\n",
    "        # Apply softmax to the scores to get the attention weights\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "\n",
    "        # Multiply the attention weights by values to get the final attention output\n",
    "        attention_scores = tf.matmul(weights, values)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = GetAttentionScores() # Scaled dot product attention\n",
    "        self.heads = h # Number of attention heads to use\n",
    "        self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k) # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k) # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v) # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "        # Tensor shape after reshaping and transposing:\n",
    "        # (batch_size, heads, seq_length,-1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads,-1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "        # Reverting the reshaping and transposing operations:\n",
    "        # (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "    def call(self,x1,x2,x3,mask):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(x1),self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length,-1)\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(x2), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length,-1)\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(x3), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length,-1)\n",
    "        # Compute the multi-head attention output using the reshaped queries,\n",
    "        # keys, and values\n",
    "        o_reshaped = self.attention(queries=q_reshaped, keys=k_reshaped,values=v_reshaped,d_k=cast(self.d_k,float32),mask= mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length,-1)\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)\n",
    "#ENCODER PART\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h=h,d_k=d_k,d_v=d_v,d_model=d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()  # First Add & Norm\n",
    "        self.feed_forward1 = FeedForward(d_ff=d_ff,d_model=d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()  # Second Add & Norm\n",
    "\n",
    "    def call(self, x, padding_mask=None, training=False):\n",
    "        # Attention with dropout and add & norm\n",
    "        # Ensure mask is passed correctly\n",
    "        multiheaded_output = self.multihead_attention(x,x,x,mask=padding_mask)  \n",
    "        multiheaded_output = self.dropout1(multiheaded_output, training=training)\n",
    "        \n",
    "        addnorm_output1 = self.add_norm1(x, multiheaded_output)  # Add & Norm with attention output\n",
    "\n",
    "        # Feed-forward with dropout and second add & norm\n",
    "        feed_forward_output = self.feed_forward1(addnorm_output1)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
    "        \n",
    "        return self.add_norm2(addnorm_output1, feed_forward_output)  # Add & Norm with feed-forward output\n",
    "\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "    **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(seq_length=sequence_length, vocab_size=vocab_size,output_dim=d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h=h, d_k=d_k, d_v=d_v, d_model=d_model, d_ff=d_ff, rate=rate)for _ in range(n)]\n",
    "    \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i,layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training=training)\n",
    "            \n",
    "        return x\n",
    "\n",
    "# DECODER PART\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self,h,d_k,d_v,d_model,d_ff,rate,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.multihead_att1=MultiHeadAttention(h,d_k,d_v,d_model)\n",
    "        self.dropout1=Dropout(rate)\n",
    "        self.add_norm1=AddNormalization()\n",
    "\n",
    "        self.multihead_att2=MultiHeadAttention(h,d_k,d_v,d_model)\n",
    "        self.dropout2=Dropout(rate)\n",
    "        self.add_norm2=AddNormalization()\n",
    "\n",
    "        self.feed_forward =FeedForward(d_ff,d_model)\n",
    "        self.dropout3=Dropout(rate)\n",
    "        self.add_norm3=AddNormalization()\n",
    "\n",
    "\n",
    "    def call(self,x,encoder_output,lookahead_mask,padding_mask,training):\n",
    "\n",
    "        multihead_output1= self.multihead_att1(x,x,x,lookahead_mask)\n",
    "\n",
    "        multihead_output1=self.dropout1(multihead_output1,training=training)\n",
    "        add_norm1_output = self.add_norm1(x,multihead_output1)\n",
    "\n",
    "        #keys and values are encoder output\n",
    "        multihead_output2 = self.multihead_att2(add_norm1_output,encoder_output,encoder_output,padding_mask)\n",
    "\n",
    "        multihead_output2= self.dropout2(multihead_output2,training=training)\n",
    "        add_norm2_output = self.add_norm2(add_norm1_output,multihead_output2)\n",
    "\n",
    "\n",
    "        feed_forward_output= self.feed_forward(add_norm2_output)\n",
    "        feed_forward_output=self.dropout3(feed_forward_output,training=training)\n",
    "\n",
    "\n",
    "        return self.add_norm3(add_norm2_output,feed_forward_output)\n",
    "\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length,vocab_size,d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        \n",
    "        self.decoder_layer= [DecoderLayer(h,d_k,d_v,d_model,d_ff,rate) for _ in range(n)]\n",
    "\n",
    "\n",
    "    def call(self,output_target,encoder_output,lookahead_mask,padding_mask,training):\n",
    "\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "\n",
    "        x=self.dropout(pos_encoding_output,training=training)\n",
    "\n",
    "        for i,layer in enumerate(self.decoder_layer):\n",
    "            x=layer(x,encoder_output,lookahead_mask=lookahead_mask,padding_mask=padding_mask,training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    def __init__(self,enc_vocab_size,dec_vocab_size,enc_seq_length,dec_seq_length,h,d_k,d_v,d_model,d_ff_inner,n,rate,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.encoder = Encoder(enc_vocab_size,enc_seq_length,h,d_k,d_v,d_model,d_ff_inner,n,rate)\n",
    "\n",
    "        self.decoder = Decoder(dec_vocab_size,dec_seq_length,h,d_k,d_v,d_model,d_ff_inner,n,rate)\n",
    "\n",
    "        self.model_last_layer = Dense(dec_vocab_size)\n",
    "\n",
    "    def padding_mask (self,input):\n",
    "\n",
    "        mask = math.equal(input,0)\n",
    "        mask = cast(mask,float32)\n",
    "\n",
    "        return mask[:,newaxis,newaxis,:]\n",
    "    \n",
    "    def lookahead_mask(self,shape):\n",
    "\n",
    "        mask =  1 - linalg.band_part(ones((shape,shape)),-1,0)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def call ( self, encoder_input,decoder_input,training):\n",
    "\n",
    "        enc_padding_mask = self.padding_mask(encoder_input)\n",
    "\n",
    "        dec_padding_mask = self.padding_mask(decoder_input)\n",
    "        dec_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_lookahead_mask =  maximum(dec_padding_mask,dec_lookahead_mask)\n",
    "\n",
    "\n",
    "        encoder_output = self.encoder(encoder_input,enc_padding_mask,training=training)\n",
    "\n",
    "        decoder_ouput = self.decoder(decoder_input,encoder_output,dec_lookahead_mask,enc_padding_mask,training=training)\n",
    "\n",
    "        model_output = self.model_last_layer(decoder_ouput)\n",
    "\n",
    "        return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences=10000\n",
    "        self.train_split =0.9\n",
    "\n",
    "    def create_tokenizer (self,dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self,dataset):\n",
    "\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "    \n",
    "    def find_vocab_size (self,tokenizer,dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index)+1\n",
    "    \n",
    "    def call( self, filename):\n",
    "\n",
    "        clean_dataset = load(open(filename,'rb'))\n",
    "        dataset = clean_dataset[:self.n_sentences,:] \n",
    "\n",
    "        for i in range(dataset[:,0].size): #Only English Sentences size i.e 10000\n",
    "            dataset[i,0]=\"<START> \"+ dataset[i,0] + \" <EOS>\" #adds start and eos in english\n",
    "            dataset[i,1]=\"<START> \"+ dataset[i,1] + \" <EOS>\" #same here but in german    \n",
    "\n",
    "        shuffle(dataset)\n",
    "\n",
    "        train = dataset[:int(self.n_sentences*self.train_split)]\n",
    "\n",
    "        #tokernize english sentences and other stuff\n",
    "        enc_tokenizer = self.create_tokenizer(train[:,0])\n",
    "        enc_seq_length = self.find_seq_length(train[:,0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer,train[:,0])\n",
    "\n",
    "        #encode and pad \n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:,0])\n",
    "        trainX = pad_sequences(trainX,maxlen=enc_seq_length,padding=\"post\")\n",
    "        trainX = convert_to_tensor(trainX,dtype=int64)\n",
    "\n",
    "        #tokenize german \n",
    "        dec_tokenizer = self.create_tokenizer(train[:,1])\n",
    "        dec_seq_length = self.find_seq_length(train[:,1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer,train[:,1])\n",
    "\n",
    "        #enocde and pad\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:,1])\n",
    "        trainY = pad_sequences(trainY,maxlen=dec_seq_length,padding=\"post\")\n",
    "        trainY = convert_to_tensor(trainY,dtype=int64)\n",
    "\n",
    "        return (trainX,trainY,enc_seq_length,dec_seq_length,enc_vocab_size,dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrepareDataset()\n",
    "(trainX, trainY,enc_seq_length, dec_seq_length,enc_vocab_size, dec_vocab_size) = dataset.call(\"english-german-both.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_model = 512 # Dimensionality of model layers' outputs\n",
    "d_ff = 2048 # Dimensionality of the inner fully connected layer\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler (LearningRateSchedule):\n",
    "    def __init__(self, d_model,warmup_steps=4000,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model,float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self,step_num):\n",
    "        #linearly increasing learning rate for the first warmup steps and decreasing after\n",
    "        arg1= step_num ** -0.5\n",
    "        arg2= step_num * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return ( self.d_model ** -0.5 ) * math.minimum(arg1,arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(LRScheduler(d_model),beta_1,beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.Dataset.from_tensor_slices((trainX,trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size,dec_vocab_size,enc_seq_length,dec_seq_length,h,d_k,d_v,d_model,d_ff,n,dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy= Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = train.Checkpoint(model=training_model,optimizer=optimizer)\n",
    "checkpoint_manager = train.CheckpointManager(checkpoint,\"./checkpoints\" ,max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of loss\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(mask)\n",
    "\n",
    "# Defining the accuracy function\n",
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of accuracy\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 Step 0 Loss 8.3999 Accuracy 0.0000\n",
      "Epoch 1 Step 50 Loss 7.6825 Accuracy 0.1210\n",
      "Epoch 1 Step 100 Loss 7.0614 Accuracy 0.1700\n",
      "Epoch 1: Training Loss 6.7397, Training Accuracy 0.1898\n",
      "Total time taken: 124.95s\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0 Loss 5.6838 Accuracy 0.2568\n",
      "Epoch 2 Step 50 Loss 5.4632 Accuracy 0.2737\n",
      "Epoch 2 Step 100 Loss 5.2908 Accuracy 0.2820\n",
      "Epoch 2: Training Loss 5.1536, Training Accuracy 0.2897\n",
      "Total time taken: 90.19s\n"
     ]
    }
   ],
   "source": [
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "        # Compute the training loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "    start_time = time()\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1} Step {step} Loss {train_loss.result():.4f} \"\n",
    "            + f\"Accuracy {train_accuracy.result():.4f}\")\n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \"\n",
    "    + f\"Training Accuracy {train_accuracy.result():.4f}\")\n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = checkpoint_manager.save()\n",
    "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "    print(\"Total time taken: %.2fs\" % (time()- start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
